{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE bac Ã  sable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : https://hunterheidenreich.com/posts/modern-variational-autoencoder-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VAEOutput:\n",
    "    \"\"\"\n",
    "    Dataclass for VAE output.\n",
    "    \n",
    "    Attributes:\n",
    "        z_dist (torch.distributions.Distribution): The distribution of the latent variable z.\n",
    "        z_sample (torch.Tensor): The sampled value of the latent variable z.\n",
    "        x_recon (torch.Tensor): The reconstructed output from the VAE.\n",
    "        loss (torch.Tensor): The overall loss of the VAE.\n",
    "        loss_recon (torch.Tensor): The reconstruction loss component of the VAE loss.\n",
    "        loss_kl (torch.Tensor): The KL divergence component of the VAE loss.\n",
    "    \"\"\"\n",
    "    z_dist: torch.distributions.Distribution\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.Tensor\n",
    "    \n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_kl: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) class.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input data.\n",
    "        hidden_dim (int): Dimensionality of the hidden layer.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 8),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 8, 2 * latent_dim), # 2 for mean and variance.\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 8),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 8, hidden_dim // 4),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Encodes the input data into the latent space.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            eps (float): Small value to avoid numerical instability.\n",
    "        \n",
    "        Returns:\n",
    "            torch.distributions.MultivariateNormal: Normal distribution of the encoded data.\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        \n",
    "        return torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "        \n",
    "    def reparameterize(self, dist):\n",
    "        \"\"\"\n",
    "        Reparameterizes the encoded data to sample from the latent space.\n",
    "        \n",
    "        Args:\n",
    "            dist (torch.distributions.MultivariateNormal): Normal distribution of the encoded data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled data from the latent space.\n",
    "        \"\"\"\n",
    "        return dist.rsample()\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the data from the latent space to the original input space.\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Data in the latent space.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed data in the original input space.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            compute_loss (bool): Whether to compute the loss or not.\n",
    "        \n",
    "        Returns:\n",
    "            VAEOutput: VAE output dataclass.\n",
    "        \"\"\"\n",
    "        dist = self.encode(x)\n",
    "        z = self.reparameterize(dist)\n",
    "        recon_x = self.decode(z)\n",
    "        \n",
    "        if not compute_loss:\n",
    "            return VAEOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "        \n",
    "        # compute loss terms \n",
    "        loss_recon = F.binary_cross_entropy(recon_x, x, reduction='none').sum(-1).mean()\n",
    "        std_normal = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros_like(z, device=z.device),\n",
    "            scale_tril=torch.eye(z.shape[-1], device=z.device).unsqueeze(0).expand(z.shape[0], -1, -1),\n",
    "        )\n",
    "        loss_kl = torch.distributions.kl.kl_divergence(dist, std_normal).mean()\n",
    "                \n",
    "        loss = loss_recon + loss_kl\n",
    "        \n",
    "        return VAEOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=recon_x,\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing, CSV & image file I/O\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/franzele/Desktop/univ_lille/m1s2/rd/RD_project'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([cv2.cvtColor(cv2.resize(cv2.imread(f\"cat/{path}\"), (64, 64)), cv2.COLOR_BGR2GRAY).flatten() for path in os.listdir(\"cat\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211</td>\n",
       "      <td>193</td>\n",
       "      <td>185</td>\n",
       "      <td>183</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>199</td>\n",
       "      <td>...</td>\n",
       "      <td>133</td>\n",
       "      <td>155</td>\n",
       "      <td>133</td>\n",
       "      <td>137</td>\n",
       "      <td>118</td>\n",
       "      <td>96</td>\n",
       "      <td>104</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166</td>\n",
       "      <td>169</td>\n",
       "      <td>178</td>\n",
       "      <td>153</td>\n",
       "      <td>98</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "      <td>113</td>\n",
       "      <td>125</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>101</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>127</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>227</td>\n",
       "      <td>222</td>\n",
       "      <td>221</td>\n",
       "      <td>222</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>220</td>\n",
       "      <td>215</td>\n",
       "      <td>211</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>145</td>\n",
       "      <td>135</td>\n",
       "      <td>126</td>\n",
       "      <td>115</td>\n",
       "      <td>148</td>\n",
       "      <td>132</td>\n",
       "      <td>113</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>179</td>\n",
       "      <td>193</td>\n",
       "      <td>186</td>\n",
       "      <td>192</td>\n",
       "      <td>150</td>\n",
       "      <td>163</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>162</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>104</td>\n",
       "      <td>85</td>\n",
       "      <td>79</td>\n",
       "      <td>52</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648</th>\n",
       "      <td>172</td>\n",
       "      <td>163</td>\n",
       "      <td>145</td>\n",
       "      <td>127</td>\n",
       "      <td>142</td>\n",
       "      <td>167</td>\n",
       "      <td>183</td>\n",
       "      <td>191</td>\n",
       "      <td>187</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>58</td>\n",
       "      <td>78</td>\n",
       "      <td>100</td>\n",
       "      <td>94</td>\n",
       "      <td>99</td>\n",
       "      <td>43</td>\n",
       "      <td>64</td>\n",
       "      <td>33</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>139</td>\n",
       "      <td>139</td>\n",
       "      <td>140</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>135</td>\n",
       "      <td>140</td>\n",
       "      <td>99</td>\n",
       "      <td>59</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "      <td>80</td>\n",
       "      <td>84</td>\n",
       "      <td>87</td>\n",
       "      <td>80</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>216</td>\n",
       "      <td>225</td>\n",
       "      <td>200</td>\n",
       "      <td>137</td>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>105</td>\n",
       "      <td>109</td>\n",
       "      <td>75</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>133</td>\n",
       "      <td>128</td>\n",
       "      <td>122</td>\n",
       "      <td>119</td>\n",
       "      <td>120</td>\n",
       "      <td>124</td>\n",
       "      <td>125</td>\n",
       "      <td>126</td>\n",
       "      <td>121</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>103</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>37</td>\n",
       "      <td>80</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>188</td>\n",
       "      <td>185</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>83</td>\n",
       "      <td>162</td>\n",
       "      <td>220</td>\n",
       "      <td>193</td>\n",
       "      <td>224</td>\n",
       "      <td>230</td>\n",
       "      <td>241</td>\n",
       "      <td>238</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5653 rows Ã 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  4086  \\\n",
       "0      211   193   185   183   200   199   200   177   177   199  ...   133   \n",
       "1      166   169   178   153    98   102   103   113   125   123  ...    99   \n",
       "2       90    18    12    13    14    18    24    21    28    57  ...   227   \n",
       "3       32    33    32    33    29    27    21    22    17    14  ...   145   \n",
       "4      179   193   186   192   150   163   179   178   162   158  ...   104   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "5648   172   163   145   127   142   167   183   191   187    56  ...    36   \n",
       "5649   139   139   140   139   137   135   140    99    59    76  ...    87   \n",
       "5650    72    72    73    71    74    74    72    73    71    76  ...   204   \n",
       "5651    60    62    63    63    64    70   105   109    75    65  ...   133   \n",
       "5652   103   101   102    37    80   183   186   188   185   181  ...    59   \n",
       "\n",
       "      4087  4088  4089  4090  4091  4092  4093  4094  4095  \n",
       "0      155   133   137   118    96   104   120   150   158  \n",
       "1      112   112   101   100   110   127   109   106   108  \n",
       "2      222   221   222   228   228   220   215   211   215  \n",
       "3      135   126   115   148   132   113   116   101    67  \n",
       "4       85    79    52    39    35    31    30    28    26  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "5648    58    78   100    94    99    43    64    33    57  \n",
       "5649    92    80    84    87    80    71    63    74    77  \n",
       "5650   216   225   200   137    69    36    17    24    28  \n",
       "5651   128   122   119   120   124   125   126   121   125  \n",
       "5652    83   162   220   193   224   230   241   238   229  \n",
       "\n",
       "[5653 rows x 4096 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f846e772140>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAz8klEQVR4nO2de9BfVXnvv08SMGpUhCBNEyRRAggKQSIgUrmJlwIytait9gw6TNPp9JzR0dOi58w47ZnjjHY6tp2p7QxTe2QcKdKiAuq0IJeqIwUChGtAbqGEW1CgUksR4jp/vL/fzmd98+6VH7n83rT7+c5kst53rb322mvt9e7vdz3PelaUUpRIJP7rY95cNyCRSEwHOdkTiYEgJ3siMRDkZE8kBoKc7InEQJCTPZEYCHZoskfEuyPi7oi4NyI+tbMalUgkdj5ie+3sETFf0o8knSppo6QbJP1mKeXOnde8RCKxs7BgB649WtK9pZT7JSkiLpR0pqTeyb5w4cKyaNEiSZL/keHPe+yxR5X3yle+ctZyXscvfvGLLv3CCy9UeRHRpefN20JoFixY0FvOwfpZx85yTGKdzz//fJfec889q3I///nPe+/NOpj2/uB1/szM63vmbdUxaV+x3Pz586s81vlv//Zvve1g/2zevFmTwMfd+6evHex7qW4zy72Yd4L3Zn3b01c//elP9eyzz876Eu/IZF8q6SH8vFHSMa0LFi1apDPOOEPS1oPy7LPPbql46dIq75RTTunSnATeGc8880yXfvrpp6s8ds7ChQu79H777VeVa72YHGi+YByE2a4jWpPnpS99aZd+9NFHu/QBBxxQlfuXf/mXLu39+JKXvKRLv/zlL+/SP/nJT6py7Ef/48q8n/3sZ136Fa94RVXuP/7jP7q0/0Hqy/NJ9dxzz/XWzwn5/e9/v0uPPxhjsH/+9V//tcrrmyCvfvWrq3JPPvmk+sB2PPjgg1Ue6+F4+rjwnfCJ+sQTT3Rpftj8DwvHYu+9967yxmN4wQUX9DzFFBboImJNRKyNiLV8ARKJxHSxI1/2hyXtj5+XjX5XoZRynqTzJGnx4sXdnzf/63zYYYd16be//e1VHr9k/CtLaifVX6R99tmnt+H84vlXmF8a/+v5+OOPz1qO9UnS/vtv6ZZf/uVfrvL419r/wvNrwK/cj3/846rcvvvu25vHdpFW/vu//7v6wC+GVDMMfrn8Odl+Z1JkarzO6TPvxWv83kceeWSX3rBhQ1WOX2W+A14/+5QscLbr+vJe85rXVHlkYHxfvE85tl7H8uXLZ72Xv8Mve9nLZr2vtOXZnKURO/Jlv0HSyohYERF7SvoNSZfuQH2JRGIXYru/7KWUFyLiv0v6R0nzJf1NKeWOndayRCKxU7EjNF6llO9I+s5OaksikdiF2KHJ/mKxePFiffSjH501j3rN9dOrXvWqLk0NtmTJkqqcr5QS1I2+Ck5QJ3JlW6qtBNSrXo66ybUV9aUvWLJd1MC+Ksv7ucmLZantfRWcfbxixYre9resDr/0S7/UpV0Dc32D+pWWEKl+ZmpSSXrqqae69Lp167q0r6RTD3PNQqqfm+sFLTMi3zGpfjc9j+/gAw880KV9faOvT/3etA75uBP+zo3LtixB6S6bSAwEOdkTiYFgqjR+wYIFWrx4saStTQQ0DbmZiNSGlMrNOKTZ4/uMQTpHGuWSgXneRt6bJi+nWy0HCrbDKRcdQkjxnXLSCcNpMX9mP7akhrefbeQzO/1kOa/joIMOmrW9LROdt5HjS3Osm+hI3b1P6UzE63xseS+n6jTxunPST3/60y5NWePmNcoabyNlCO/F+iTp9ttv79LeV+P3uOUJmF/2RGIgyMmeSAwEOdkTiYFgqpr9ySef1IUXXihpa91FzeTakCYZahXXPjR3uDsuNfBee+3VpV270dzDTQl+HbWag8/iGzOoqVy/0sTI56TmlWoTmK9bEDR5sW5vl7sds7/d3EZQU7qpiWYoukI7+Jzse2/zpBtmrr/++iqPazd8J1zbMs/XQVzDE3Rpfe1rX9ulXdtzTcPfHZpP2S4vxzWYhx+uPdNf//rXb1WXI7/sicRAkJM9kRgIpkrj582b19Elp330wHLPMlITUlOXAqScLgX6ggyQmnsdNJf4dTQHvpitu6Sj9HCTatrGe7kJZtOmTbNeI9U0kDLEaSWpu0sZUk5SWqeIHDOXE5QC99xzz6z3ldomRraDJlL3kmuZ1Fg/3wGXRhwLz2uZ5SjFuBvP3x162nn7KUtYn0sN9t2hhx5a5Y3HelftekskEv+JkJM9kRgIpkrjf/GLX3SUy8P2kMK5dxBpD6mN00quqPrqM6kY0057WjHA2A62172l6F3nUoPXuacg783+4cq2tPVqMcE2k+J7n5KCOzXlRhO2w4Mp8Dk90Af7n7TY+5TlfDMNy7ZW4ynZ3vKWt1R5l112WZcer1hLW3tOchOVyyv+7EEp2Cd8H/3dZBudxrMsx9ZlDaWjbxoa0/jWJq/8sicSA0FO9kRiIMjJnkgMBFPV7NIWDdjygnKvLXpWURu6GYca1b3TqIV4b7/XEUcc0aXdg47anPqptdPK9Rmf8+67767yqJVZjgE3pbapiZqyFUqa9/I29rVj48aNVTn+vH79+iqP92Z73bOR5bwdNL2xTY888khVjvrbteyqVau6dGs95rTTTuvShxxySJXHdYvPfvazVR7XGRhctBWn38eMdbSCW3LdxddIxuPe8njML3siMRDkZE8kBoKp0vhFixbpuOOOk7S1aYJ00SkWvaBoJnKqRLrVCoDROkKKFNGpKQMy0JTidJ/U1OPi0RvOzTj0jKNnX8uM0zq6if3j9LlF9x56aMtBP9ww431Kmu2mvVYAD4ISzceiLy69l2M/ujcjJcTBBx/cpd/1rndV5Whec7Mw+5ibXRzsY5ep7EeXGrxfK74g6b736fisAvccJfLLnkgMBDnZE4mBICd7IjEQTD3g5Fg7u9mMZpdW8ICW+YR5rotolqP29N1JbIfvLDr++OM1G9yUQg02XqMYg3rYXRup/anfHa1TUWm6oW709QH2gd+r78jm1nHI/izUyrzO1w6oxd30Rv1KLe47Jnlv17lvfetbuzSDVrYCM7orLd+lN77xjVXe1Vdf3aXZj60gEr6+0XfGnwdNbZ1fOH6PW2fWbfPLHhF/ExGbIuJ2/G7viLgiIu4Z/f/qVh2JRGLuMQmN/7Kkd9vvPiXpylLKSklXjn5OJBK7MbZJ40sp34uI5fbrMyWdOEqfL+kaSeduq64XXnihoxtOSWiSclNWH01z8xHzPEYXzRakW+5pRxPJr/zKr1R5pF+8t1MnlvOdXPR8OvDAA6u8m2++uUuTZjrlpLnK+5HHRZOqH3PMMVU5ttlNjJQllD8ueUjVJ93J5UdMc9ebx55veQoSHAuPd3fUUUd16T5PNakeMw9owvbz6GhJuvLKK7s0x8klTytIh++MHMNNne9///u7NMdZ2tKPLpOI7V2g26+UMj7E7DFJ+7UKJxKJuccOr8aXmdWc3tPkImJNRKyNiLUeaTWRSEwP27sa/3hELCmlPBoRSyT1Lh2XUs6TdJ4kLV68uFx00UWStj6BlXTOvYD64oj5aZ6Er8aTOpFiOY0nJXQ6xzq4Yu3x10gRXWqQqvK4KscVV1zRpZ3m0TrhVO+xxx6btR1r166tyrH9bv1gm9leHxf2v8dt66P4vkpNWu9UnWWZ5/29cuXKLv2Od7yjtw5aJFpebC69WkeCsX+Y5zKBY+bBQrgCT4npAVi+/e1vd+k+z0mOv2N7v+yXSjp7lD5b0iXbWU8ikZgSJjG9/a2kayUdHBEbI+IcSZ+TdGpE3CPpHaOfE4nEboxJVuN/syfrlJ3clkQisQsxVQ+6hQsXdvrKdydRh3ketTL1lGvN1hHCLEu9xuAG0tbak6C+p+5yjdfSly2z3IoVK7r0Bz/4wS598cUXV+X4LG6W69O5rcCXraO42FcM7CFJ69at69LuWcYgmdwteNddd1Xl+o6Hlmqtz3UE1+X0kmOMeqnuD65vuGZnn/p7RW9PN6lxHYDBK1yX89laR3XT7NwK8OJzZGwizYCTiUQiJ3siMRRMPQbdmGY4RSE9crMFTR/cBOHeWKSETmf6Ahy4t97v/u7vznpfv/d3vvOdLu3HM7H9br5zcwrBdtHTjp5TkvSNb3yjS7e86+ih59SRcM849kkrZh7hHnSMn9YX/12qTameR6p9+umnd+nly5dX5egB6LKJpii+Ex78gePiNJ7td3pOD0aOn292oZzw94p1sO+dqvO98o1k4zpap/rmlz2RGAhysicSA0FO9kRiIJi6Zh/DdVFrBxVNJtQkXo761TUZtVbLNZI60XUXcdJJJ3Vpd4llnHfXVmyj67++PA9iQDMXzV/eFpq/XP8xAKf3FX+mCdO1MndY+dHRNCexjsMPP7wqx7UU15t0J+a93fWXWtbdWanFObbeH+x7d6Hm+tI111xT5fG5Wa5lzvT3he8Z2+HvH+v3d2dcf5reEolETvZEYiiY+pHNY9rmdIuUxc1yNCeROnossieffLJLO8VnPHiaynz3UOto4D7ziZu1li1b1qXdNMZjmPzepJykgU45GaDB6yB1Jw32cgzC4JSQZdl+j2N37733dun77ruvyqMJiTLMKTLH0N8Jet5xXLjLTaplh3vQMYgJ7+3x33nvlsmVwSq8XZRG7mnHn93USc87tsOP7GKfuvQa0/hW7Lv8sicSA0FO9kRiIJgzD7qW55cHZOAKI8v56i3r9NV+UizmOXUkZW5tMmHaV0DpEeXPwp/5LFL/Jh+PM0cpQ6or1dKDVN1XgNlmb3+fVcBlE2WCx+vrC0ftFJaWhvvvv7/K43PSe8+9EjmGPOJJqq0ClEYeo5Ay0t+d1nFb9BSkVaDlsXjAAQdUP7O/Wb/HkyN1dyvSWE40w3335iQSif9SyMmeSAwEOdkTiYFgqpp93rx5nTZy7UN94oEkqfOo/3zHGvVTKxBCn0eeVOtoN0kxrju1YCtwpEfUpa7z2N+8H9cYPNACNap717kmnu0aqT94ppdtaXte1zo6mn3sgUapPV2HUjuzr1rx5d082Bfo4w1veENVjs/WOo+AnpOSdP3113fpllmV6x3+bnIM6XHpdfDZ+vJaR3Hnlz2RGAhysicSA8FUafzmzZs7uuHeaaTWbnIgzaFpwWk2KVCLzpFKtzZEMICEJJ188sld2qkkQROJy4TW8VWsk7TSTV5so9dP772bbrqpS7/5zW+uyjWDHPRspnCzTutE3b5gJO5BxwAk7tXGdlDa+diyDh8z9l1rIwz73oOn3HjjjV2am5ykWl5Q/rj04v2cxvM9aJnsWM7Ng+OxSA+6RCKRkz2RGApysicSA8FUNXtEdJrCTUY0jXkgBGocukq6KyrdGlvH4vLerUAFrlGph6h5WzrLXW6p3VzX9WlKr4Nw3U/XWq5hfO9736vK0ZV233337a2z5RZMtLQix+X222+v8jgW7hbcZ2pyTU2Tq+9YY59ynLxPqYF9x+QJJ5zQpb/yla9UeQxo+brXva5Lt9Z0fL2Az8b3298/vsM+7uM275Bmj4j9I+LqiLgzIu6IiI+Nfr93RFwREfeM/u8/ZTGRSMw5JqHxL0j6ZCnlUEnHSvq9iDhU0qckXVlKWSnpytHPiURiN8UkZ709KunRUfqZiFgvaamkMyWdOCp2vqRrJJ3bqmvz5s0dtfQgA6SSThdpdqF5w73CSOvdPEPPuL449FJNfT1AQJ+Jx6lji9bz2byNpGCklR7EgEEuvA7m0SzkkoHtd+pH+sg+aHnJeV7fde6BRq85r4Peh3wHnMZTUrlXJfubFNlNj48++miXdorMuHN+b76DfD+8v/vMqlL93K2jtGmSdhm8YcMGSTsxbnxELJd0pKTrJO03+kMgSY9J2q/vukQiMfeYeLJHxCJJF0v6eCmlWkErM3+OZv2cRcSaiFgbEWvdmSCRSEwPE032iNhDMxP9q6WUr49+/XhELBnlL5G0abZrSynnlVJWl1JWu8dbIpGYHrap2WNGUHxJ0vpSyheQdamksyV9bvT/Jduqa/78+Z3ucPMDj0p2swX1H/WU19HaWUQtzrTvwqLuYgBLqTbxUKu1ggu2TCHu2sk2c/3BI7OwfjcT9UVLcY3He7l78qRmRY5FyyzXqsPbTzB6DPvD10j4bF7fWMtK7TUMRrjxM+3YHz5mvB/XH/ydYF+dcsopVR4Dd3JtydcHWKcfLd63m5SYxM7+Nkn/TdJtEbFu9Lv/pZlJflFEnCPpQUkfmKCuRCIxR5hkNf4Hkvr+XJzS8/tEIrGbYaoedKWUjpq4WYsUxWkaKRwpodfRMpvx+CB6OpHmSfWuJvfoIp1u0XhSNvfUYhvdTNRnEnTzGp/Nj4DuM+O45GkdbUW0zGuksK3AFryXU3pKCK+f17WOQ77rrru6tAc+Yf9wnLzvKR1dAtIE6OZeovUsNKO52ZmUnP3jcpbx5b0fx/fLXW+JRCIneyIxFEw9bvx4ZdNjs5Hu+kopaRXpi9N9p6rEihUrujRX4EmNpJpSOV3sswT4qilNjC4nWNav8+cZo7Wa7V5WlAlso292aW3UIFqWBea55YJgf7uHF/vUNzaxLGXNrbfeWpVj0At/B9jGSU8A5rvibXzb295W5XEjFd+lyy67rCrHI8e8jXzfW2cf0FLkZmx/z2ZDftkTiYEgJ3siMRDkZE8kBoKpavY999yzC4joWpm61DUqzSfUJm7uoQ5rmaRoytq0qfbyZZ6fyUWzS2t3EvWwa9lW4ES2kfdqmVNaATMZe761W8vRt0bgZsSHHnqoSzMoo1QHATn99NN778Vncx1KrcwgEa9//eurcuxv7yv2dysGPvW8x6/nu7p+/foqj33MmP1HHXVUVY5rJh7bn2tDNAn6bkeOmZ8T6GVnQ37ZE4mBICd7IjEQTD0G3ZguOVUkFXMTFE1vpHqtI5WditG8QTOO0zJSKqd6fUfrtI6O9mv6juf1PPaBU3D2lZsp+dyte7U8+SiVWId7ljFWoJua2I88aom0V6rpvkuSBx98sEtzE4v3aSsYCX9uHWfMfvO4hIcffniXdsnGsszzzUWk2S4xORaURm4ebZk6x/dLD7pEIpGTPZEYCnKyJxIDwVQ1+/z58ztTiJsOqN1aZiJqfdc+LbdSlqXp4/jjj6/KPfLII12aWkqq9WvLNMY8uklKte5tmb9olnNXSF7nbaRuZB2uV7mDr3X+Gs2PHnv+2muv7dJuTmJffeYzn+nSn/jEJ6pyDBrhdVBH07R34IEHVuW4btEK/sm1lZYebrnLOjywyBg0FUr1++0m10MOOaRLP/DAA13az0+gCdDzxu9IazdjftkTiYEgJ3siMRDM2fFPHmSAHkZOTbkbqu8YX0n60Y9+1KVdJpCq8l4PP/xwb3ud3h522GFdum83lVQHvXAKTlrppr0+M5ebGEkRXUKQxpLS0aQjSatWrerSbq76kz/5ky7NYB5Okdl3vouMz00T2pe//OWq3F/8xV90aae+lBPs0zvvvLMqR1rvR4GzjykP3eOMY+3UnDLBx4x9zPeW8kSq+8ffF7aLlN5lKeUEj+aWtoxNK6hrftkTiYEgJ3siMRBMfTV+TMecbnEVee3atVUeKTkp1Q033FCVI931VUlSd1JkUn+pjk/ndfTFQXOq3gpAwJX01kYePqd7S1FCeChsesORuq9bt64qRwl03XXXVXm33HJLl/aVY4LP4p58lF7sH4ZNluo+dYsBqSrH5aqrrqrKkQa75yTv3ZJQDBP+9a9/vco766yzurT3B+vhO+HSi+3w94VSzOcFQRrfZ3lqhfTOL3siMRDkZE8kBoKc7InEQDBVzb5gwYLOrOF6m7qU5gep1tgMNuFBFFmHe6dRD7MODyDIo3vdvEE9TC3uppTWziPqLvcUJOgh5TqU3l9f/epXq7z3vOc9XZo7xdwr7C//8i+7dF8McqnW4r7rjX3seX1ebR6LnzrXj11605ve1KVbRypzPeKEE06o8vrOGfD1AT6LBy3hu+NjS6+/1s489o+vb/Qdc+VmNLa5z/tyh3a9RcTCiLg+Im6JiDsi4o9Gv18REddFxL0R8bWImD00aiKR2C0wCY1/TtLJpZQjJK2S9O6IOFbS5yX9aSnlQElPSTpnl7UykUjsMCY5661IGrsG7TH6VySdLOlDo9+fL+kPJf1Vq65nn322837yY5dI7+i1JdVHN5E+e5zxgw46qEv7RoH77ruvS9Ms4nKCXlDvfe97Z30OqaZUvqnCaT3BNre8zmiC4eYcb7ObcX7wgx906ZUrV3bp+++/vyp39NFHd+mNGzdWeQy8QLOne+Gx/f4sfXHV3LOMRyG55yTfEZrG3LORFJ9UWuoPWtI6Omzp0qVVXmvMaMLkWQj+TrCcB+kg9Wa/eR00q7kMGUulHQ5eERHzRye4bpJ0haT7JD1dShnPmo2SlvZcnkgkdgNMNNlLKZtLKaskLZN0tKRD2ldsQUSsiYi1EbHWF5oSicT08KJMb6WUpyVdLemtkvaKiDFfXSZp1h0lpZTzSimrSymrW95BiURi12Kbmj0i9pX0fCnl6Yh4qaRTNbM4d7WksyRdKOlsSZdsq67nnnuu02juTkiN7S6JNMm4JiOoS12Hsv6LL764S7eOCb7ooouqvCOOOGLW61qmN9dd1MO+24walRpy8eLFVTmapI499tgqr2+9wE2M1KE0w0m12YhBP/x8Pmp4H88+c5u7c1LP/9Zv/VaVx36kRuXzS7WO9naw/2nqdPMX4WZEvo977713lUdzL5/ZA6uwXb6rjm3kuLirdevMhLGJsRXAZRI7+xJJ50fEfM0wgYtKKd+KiDslXRgR/1fSzZK+NEFdiURijjDJavytko6c5ff3a0a/JxKJ/wSYqgfdvHnzOtrmcbVJX9zLqu8oZqeVNMn4bjbGMONCodN4Hv97xhlnzHpfqaZ6Tp3p3eRUrGWW89hqY/hzcgeY9w0pLSWJe+uxXR//+MervHPPPbdLkxZ6sBDKKzcFsSzzvBx37fmzsI18Lpd5pOT+nKTdzHN5RQruVJ0el37OAGMMkqrTHCjVplQ3HfaZKT1YCKm7e+iN8zJufCKRyMmeSAwFU98IM6ZI7v3GY4HcHk9qQnp46623VuUuuWSLQcC9sbwdYzhVoqeWe9dxFZhUz6lpK7AF4SvHpLGUHS5Jzj777C7dCnPcd4yTVFNJX+3vOy3Un6UVrIEyjVLAJRqvc+80bgRhm/zd4Xi6BWXNmjWz1uGWEPajywTGq/OYhRwzni7rco3X+UYbPidpvK+48z1zuj7Oa63G55c9kRgIcrInEgNBTvZEYiCYqmbfvHlz58nm2o06yXU0TSuMO+6amprG6+irr2XmawWSpCZzfcafXVtRU3ke78edfu4NyB18vouMnoK8V0tTt8xQ1PZ+tDPNTt7Gvnjq3t+8l5sYOZ5cx3HN+/3vf79L84gkqTa3NY9Ggj72oBEMYuLPSU859qOPLYOHuNt43zFdLTOav3N95mkiv+yJxECQkz2RGAimSuOlLdTEg1eQbrk5iccAcfOFx+FqbRQgNSON9w0R/NmPI+IGmt/5nd/p0u4N2KJfRCsOGgMo0KtPkn74wx92aae0pNrs05aXn3udke4y7SZRUkd/ZpqrSHXf8IY3VOXYBy2zHE2uvlGFnnzvf//71Qfey5+ZcsIlIKXBO9/5ziqPkooyxOVEXyw8qTYDclxcepHu+3iOn61lis0veyIxEORkTyQGgpzsicRAMHXNPgZNS1Idf9s1GTUNAyXedtttVTlqmr5dQVKtmdxtshWUgrqRJq5WHe6+SPOMB8WktqW+dHdWPvdv//ZvV3nUfGy/uw+zPxiY0tvcOuaY2tZj7FNv8ll8jYFj9slPfrLK43oBzwjwwBDcnehmM2pq7rDzMes7E06qn9N3/h155Jbd3wyewvPypHoNphXshO++m9PYRl+vGq8b5a63RCKRkz2RGAqmSuOfeeYZffe735W0dVAHml38+CeankjPuSNLqk1lrSNz6cHkZpY+jyipDkhA2eGeZaSBTglJp1uxyHid033W4V5hpNqTxra/6667qjxS4SeeeKJLO3XkrkM3m5FOksL+/u//flWO/e9x6fmOMM9pPCXJ8ccfX+X98z//c5f+0Ic+1KX9/eB4cuejVD93yyOS76mbY3kWgktMvj/Ma3k9uml5EnNvftkTiYEgJ3siMRBMlcYvXLhwq40bYzBM82GHHVbl9QWscLpFmvPAAw9UefSMI011Gky67x5jXDn+9re/3aV//dd/vSrHwBZOt1oeTn0x4/wa0v/WSZ8MmODeemyXn4ZLmsyADN6nlFE+FgxEwc00LpsYiMKlBq9jcBNfEadX3vXXX1/lrV69uktzBd5pL9vvHm5sh29iIY334BsEA5+4ZaTPAkTrgVSvzvs7MX53/H0j8sueSAwEOdkTiYEgJ3siMRBMVbM///zzXQxuN+NQG7oZh7r0wx/+cJf+/Oc/33sv33VEzUq94yYSwrUhr2P68ssvr8rRy++Nb3xjlUe9zd18Uh0Ygf3jupyBEFwnsu8YX55HI0u1Sc1Ne31HEHk8dZqaXCvSo47a3s2UXFfwPHr2sR99FyD70c22vK4VzKMVY7/Ps1Gq+6rveGip/c71BQZtBTL1cR+PoV9DTPxlHx3bfHNEfGv084qIuC4i7o2Ir0VEf2iYRCIx53gxNP5jkngcxucl/Wkp5UBJT0k6Z2c2LJFI7FxMROMjYpmk0yR9VtInYsZWcLKksUvS+ZL+UNJfterZY489OnpNOi7VppWtGglzBOn5+973vqoczWFuNqP5hHTLNyWQSjp9Jr0jXXIvOXpLOUjhPP456ydNI+WWannhG0tI/+nx57SP5h6PhU4KetBBB3VpHyMGa3D6SMrPe7XOBHCzHCUE+60ljZw+c6w5Ti4j+7wXpXpzilN8PjfLuWmP71IrPiLb5X3FPH/HxmcLtM5LmPTL/meS/kDSWHjsI+npUspYYGyUtHSW6xKJxG6CbU72iDhd0qZSyo3bKttz/ZqIWBsRa/1LlkgkpodJaPzbJL03In5V0kJJr5T055L2iogFo6/7MkkPz3ZxKeU8SedJ0pIlS/rPpkkkErsUk5zP/mlJn5akiDhR0v8spXw4Iv5O0lmSLpR0tqRL+uoYY9GiRTrmmGMkbR2QgXDdxV1ZPNaYJi5JWrduXZf2YJHUbq3jlmlCcnMS9TxZih/P++CDD3ZpHvcr1XrN1wSoG6l5XV/ecccdXdoDHNAMxbS3kWYc1/3Uylxb8YAMxx13XJf2Y4g5hnwu16EXXHBBl/a48ew77pzz3Y4MhOJ92jr7jOC4tIJXuBbvc3/2Oqj1Wy6thM8DvtM8C1CSbr75Zklb9y+xI04152pmse5ezWj4L+1AXYlEYhfjRTnVlFKukXTNKH2/pKNb5ROJxO6Dqe96G+9o83jtpNPu3USaSWrqFJZUz4/dJQWix5LTslYQAJpZaApysxO92DxIB72xfAcVaRvNa05NeexSa3ffvffe26Xdo5DwnX+koN/85je79Be+8IWqHGm3yxXSW3r8/fEf/3FVjv3j1Jd9xV1jrE+qn3l7gjo4Jn0HpPo5eZ3XwT72Ovqu8/7gWD/yyCNV3jhoic8dIn3jE4mBICd7IjEQzNnxTx4ggPTL6TkpEOmiewu1QkmTdjOvFZvNPZ1IrenV5hs46K3m7eBqqeeRrnN128uxXb7xgxSRp5u69xsDVnj7999//y7No5X+6Z/+qSq3atWqLk1pIdUWj0svvbRLuycfY8b5SjLH+tBDD+3SvkGJ9Jabf6Tto/GtFfxW8BHey9/vVpt4P46fW4pI3d3a1NoAM0Z+2ROJgSAneyIxEORkTyQGgqlq9j322KPTdq6HqYXcJEWdR33pXlvU336EFDW763SidQwQ20ht6HqJaw5uCqF5rXU0FD2ufKcV+6DlAfiRj3ykS9MMJ9XrIDRZSvUxRieddFKX9njtXLegZ6PUv/OPOl+qn82vOeuss2at3/uDpjjvU/Z/y+RKuC5nHX1HJUuTe8a17s02+nO6FyQxbnNrvSG/7InEQJCTPZEYCKZK40spHSV3Gt8yW/TFB/NTXHmdm5oYZ400zT35SMucbpGmUVq4lxwlQ8v7zWkf+4TU0dvYFwvP6yQtpulKqs2U3o63vOUtXbp15BAliW9fJuXnvd1cSrMZveRa9/Zn5pj5piGe4srNRX70Fsfan9Pr7MvbHjPfi7mOR3s9+eSTVd743c9TXBOJRE72RGIoyMmeSAwEU9Xs8+fP77RS60jlljsh9bsHYqSOcRfQPvOJ62FqMF874Hlg1H9uyqPbZytAhe82o47mdd4O9o+b/WhiO+qoo7q0m3H6zr6Tar3Ne/u9+CwM2OFlaYr0Os4888xZ7yXVayEXXXRRl/7gBz9YleP74a7WNJFyXcHXDnb0OOSdBd7Ld27yvXW3Y3+XZkN+2ROJgSAneyIxEEx919sYTtlIM91LidSMQRh4VK9UU1iaKaTaFNfaIUS6795IpJVsU4uqt45sdspJas3+cZlAqeEmTNJWPovTPNJAN5uRPvI6p7Os3/ubfcc+cBMg+86pNYNjkLp7DDpKthadpanQzXf0uHRpt6tpPPuK6S9+8YtVOb7f3v7xO5IedIlEIid7IjEUTN2Dbkz9Wt5pDtIqesIdfvjhVTnSYg9iQG8ylwkEgzo4ze47cdTLtYJjkJ77xhLSaV7n7eXPHm6YEoV1eJhmXue0lX3cR8el+rm9DsoEbmxyCwqlgG/IcS+3Ma655prq51NPPXXW9kr9p+H6s7D9TpFb3nU7G9zs4qfrtrxMJ5Ea+WVPJAaCnOyJxECQkz2RGAimqtkjotMa7tFFHdrSH9RWjz/+eJVHLygeBeV1Uif67jhqTa+fZWnycl1OzedmOT6ne0jx2dg/bl7jdS1zGNvhO7f6dvBJtfmK5TwgCO/dCp7JOlxTU8MvW7asymO7uObwa7/2a1W5SU2dbH8reErLg6713vZd82Jw7bXXdmnX7ByX1lj0YdLz2TdIekbSZkkvlFJWR8Tekr4mabmkDZI+UEp5qq+ORCIxt3gxNP6kUsqqUsrq0c+fknRlKWWlpCtHPycSid0UO0Ljz5R04ih9vmbOgDt3WxeN6YbTypYHHc1JjI/mRw7RNObx1BnEgPU7raR3llMlbnChKc+9tlpmnJZnXB8Vc8pJTz6nlX0x0Zyatkx7lAKk0v6crTHr8+Ty/qA0agXioEnUy9EE6JKE13FsXV5xXLx+yiZKRal9XFMfvBw3cDE2vJse6aXo7R+Pxc4IXlEkXR4RN0bEmtHv9iuljI2Cj0nab/ZLE4nE7oBJv+zHl1IejojXSLoiIu5iZimlRMSsf9ZGfxzWSNJrX/vaHWpsIpHYfkz0ZS+lPDz6f5Okb2jmqObHI2KJJI3+39Rz7XmllNWllNWkVIlEYrrY5pc9Il4uaV4p5ZlR+p2S/o+kSyWdLelzo/8v2VZddJd1jUcN77uw+nZe+flixMaNG6uf6bJJ7e0ah+V8FxZ/Zvtd21PztXbYubssj6OmNnSNR73m7ae+p35tuSe34ujzOtfDfE5fV+CYcWx9p2JrV11f8E93FeUaiZ85QPdTfmya2rbhStvCpOY2L8fAH1xXcNMsr/Mxm2S9YBIav5+kb4xutEDSBaWUf4iIGyRdFBHnSHpQ0gcmqCuRSMwRtjnZSyn3Szpilt//RNIpu6JRiURi52OqHnSbN2/uvIL82F3CzQo85omeVC4F7rzzzi7ttIbx45YsWdKlnSrRvMFyUm3qI231OihDfEcczWZ+fFVffHKP18f7MRaeVNNA7mzzuillnBaz73id9ykpuI8ZdwWuXLmySzuFnZRO895uGmMbW+Y7jlmrXEvyeB9ManprSTua2ChDfKci36tWnMY+pG98IjEQ5GRPJAaCnOyJxEAwVc2+YMGCztzkpgPq79ZZbzfddFOXXrx4cVWOOtHNYdSy1JCuebnTiLpW6g846SYp5rlJitrKtRt1Xd/5dlKt1zz4IrUzNZ+vkbBdvq7Q59LrUXFoHmQg0Nl+nq3t20JflJwXc+YAn5v1tcx8/v4R/l6xT9hX1113XVWOY7hhw4Yqjz/T/OprQRwzX6/yd2Q25Jc9kRgIcrInEgPB1OPGTxKwz2na3Xff3aWPOeaYLn311VdX5Ujr3fuN9NyPKiIOOOCALv3QQw9VeaRKpPhOb0mxnG6R8u+///5VHmkb6adTdXreOT0nlSTl9P6gV1hrByKli7s7M6inm96InXGUccv0tr2ea0SLuhMuvdhXfB+9vxnznTvbpK2PXx7DjzBr7VQcz6s8sjmRSORkTySGgqnT+DEdc5pDWuwr5FdccUWXpgw4+uijq3L33HNPl16xYkWVR+83bqCh151UU6XWqaKt1WFSLKd9++yzz6z38vuxP3xFn55UTp/Zr1zRdw8r1uGUkHSd96YVY7Z7EzvjyKQ+Gt/naTgbOE6tOHOTtsPHgkFM+I75O8FybuXhs3EsvBwlg/f9uF1J4xOJRE72RGIoyMmeSAwEU9Xszz//vB577DFJW8drpyno5ptvrvKOO+64Lv3DH/6wS1911VVVuTVr1nTp9evXV3nUWrfddluXPvLII6tyNFe5lmUdNOV5QAYGTHCzWct8QtMbyx144IFVOe6M8kAffWeb+RoJ9fzy5curPN6vb51iW+jbhbWztbzfy/P6gotOGhzS4Ws8N954Y5dm37vZlv3tY0ZPxFacfo6Frx2k6S2RSHTIyZ5IDARTp/Fj84Qf3Utzm5tWSHPe9773denLL7+8KkdPNq//zDPP7NIHHXRQl3bzF4/fcbMZqRjNd4xJL9XmNd/M0IIHyxjDPan4nE7nSN0pIdxMSbRi+THAhnuusX88xrnHmB+jRZ93hidcK7hE6zpuGvKNQfS4vOWWW6o89h03tDgF7zvCWqrNdHxfWmcOuNfmeM60PAHzy55IDAQ52ROJgSAneyIxEExVs5dSOpc/19Q0lflOLppPDjvssC592mmnVeWo9X2HFvUay/n6ALWV6yK2i0EGDj744KoctfLSpUurPJ5V5xqdu/aoh931l7umPPY8n4e76jz2OV0x/XjrPvOj60HmuXsozUtsk68xcFy4i06q13FawSXYXtfs3DHJd86DfVIf+3luNId5UA6aYBms1PU2n9vXM/rMsb7GQHdZb+P4efy+RH7ZE4mBICd7IjEQTH3X25hmeMws0o8HHnigyiO95XVu7iHdcmpNCkfTh5vGWmYtmsBIU/3ASlIsN9/x3k4lWT9NaL7Dic/tXm30smI73KOLUsMlFesg5fRn8dh7BPun5RVG2upejxwLygSXeaTu7rHI+/Edc/MXzaWex/Z7/aTurdjz/q4SfAdZB8dBqiWby5XWMWNd3dssISki9oqIv4+IuyJifUS8NSL2jogrIuKe0f/9pz4kEok5x6Q0/s8l/UMp5RDNHAW1XtKnJF1ZSlkp6crRz4lEYjfFJKe4vkrS2yV9RJJKKT+X9POIOFPSiaNi50u6RtK5rbrmzZvXUdJVq1ZVedyc4pTzxBNP7NKkuq1ji3wl/dZbb+3S3OjhNP6MM87o0tzQIqnbxCPVK90MWuD3dqrb51nmefTMcjnhG2gIen9ddtllXZqrxlKbLrJPKCH4/N6uVmhjjouPGam1rySTgjPtHm5sh1tX+Jy0frgUIEV2ywKpu7+bbAvfTbd+sE9bwU4oIfwdpjTy8Rx7hbber0m+7CskPSHp/0XEzRHx16Ojm/crpYxnw2OaOe01kUjspphksi+Q9GZJf1VKOVLSz2SUvcz86Z7V6Tki1kTE2ohY64sbiURiephksm+UtLGUMj7i4u81M/kfj4glkjT6f9NsF5dSziulrC6lrG6d3JpIJHYtJjmf/bGIeCgiDi6l3K2ZM9nvHP07W9LnRv9fMkFdnZZzfUY95aa3Po83Nz/wZ9ehRxyx5Yh5ak/GiffrXP/Qc4075+gV56DHn1R73nlgTZpraNLxdrAPPNAH1yb6tLdU60Tf+UedS5OOjxn1qpsA+wJFvJgjj2muWrZsWZf2dRCOmQfpYGARroN4n7JOBi6Van3saxP8gNGU6mZV9r9rdppImedHNvPYcQ/+MonpbVI7+/+Q9NWI2FPS/ZI+qhlWcFFEnCPpQUkfmLCuRCIxB5hospdS1klaPUvWKTu1NYlEYpdhqh508+fP70wcTvtIiz1GF80RbnYhWKebq0jbSIfc04k0yqneCSec0KVpCvLNLlyIdMpGyul09JJLtighSg0v16K+NG2xP1qedk4BSVVJR/1e7AM3qZG2Ms/7g33seZQrfOZW/EKXb32nuPqRS9wY5O8fzYgt0yHv1Qpa8qY3van6uS+Gnns29m3mmhTpG59IDAQ52ROJgSAneyIxEExVs8+bN68zM/jOIuokD8hAUF+6GYT6qeVSSZ3Y2uzfCvhHTera3gNEEtR/rqMZ2JDt9XKtNtOESTOU7zZjmz2vT+u7Hm7p7b72+tjyOq+fpjcGtuAuSKl9th61M98xNzfyZ383medt7Dv6uuXS7O8LtTjNfN5GrlX0rR207ptf9kRiIMjJnkgMBLG9x+Bs180intCMA85iST/eRvFdjd2hDVK2w5HtqPFi23FAKWXf2TKmOtm7m0asLaXM5qQzqDZkO7Id02xH0vhEYiDIyZ5IDARzNdnPm6P7ErtDG6RshyPbUWOntWNONHsikZg+ksYnEgPBVCd7RLw7Iu6OiHsjYmrRaCPibyJiU0Tcjt9NPRR2ROwfEVdHxJ0RcUdEfGwu2hIRCyPi+oi4ZdSOPxr9fkVEXDcan6+N4hfsckTE/FF8w2/NVTsiYkNE3BYR6yJi7eh3c/GO7LKw7VOb7BExX9IXJb1H0qGSfjMiDp3S7b8s6d32u7kIhf2CpE+WUg6VdKyk3xv1wbTb8pykk0spR0haJendEXGspM9L+tNSyoGSnpJ0zi5uxxgf00x48jHmqh0nlVJWwdQ1F+/IrgvbXkqZyj9Jb5X0j/j505I+PcX7L5d0O36+W9KSUXqJpLun1Ra04RJJp85lWyS9TNJNko7RjPPGgtnGaxfef9noBT5Z0rckxRy1Y4Okxfa7qY6LpFdJekCjtbSd3Y5p0vilkhh0e+Pod3OFOQ2FHRHLJR0p6bq5aMuIOq/TTKDQKyTdJ+npUsp4Z8q0xufPJP2BpPFumX3mqB1F0uURcWNErBn9btrjskvDtucCndqhsHcFImKRpIslfbyUUkX7n1ZbSimbSymrNPNlPVrSIbv6no6IOF3SplLKjdO+9yw4vpTyZs3IzN+LiLczc0rjskNh27eFaU72hyXtj5+XjX43V5goFPbORkTsoZmJ/tVSytfnsi2SVEp5WtLVmqHLe0XEeI/kNMbnbZLeGxEbJF2oGSr/53PQDpVSHh79v0nSNzTzB3Da47JDYdu3hWlO9hskrRyttO4p6TckXTrF+zsu1UwIbGnCUNg7ipjZuPwlSetLKV+Yq7ZExL4Rsdco/VLNrBus18ykP2ta7SilfLqUsqyUslwz78NVpZQPT7sdEfHyiHjFOC3pnZJu15THpZTymKSHImJ8BPE4bPvOaceuXviwhYZflfQjzejD/z3F+/6tpEclPa+Zv57naEYbXinpHknflbT3FNpxvGYo2K2S1o3+/eq02yLpcEk3j9pxu6TPjH7/OknXS7pX0t9JeskUx+hESd+ai3aM7nfL6N8d43dzjt6RVZLWjsbmm5JevbPakR50icRAkAt0icRAkJM9kRgIcrInEgNBTvZEYiDIyZ5IDAQ52ROJgSAneyIxEORkTyQGgv8PyEqW6nzLC4oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(data.iloc[1, :]).reshape((64, 64)), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "num_epochs = 50\n",
    "latent_dim = 2\n",
    "hidden_dim = 512\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(input_dim=4096, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "writer = SummaryWriter(f'runs/mnist/vae_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, prev_updates, writer=None):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for batch_idx, data in enumerate(tqdm(dataloader)):\n",
    "        target = np.zeros(data.shape[0])\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        output = model(data)  # Forward pass\n",
    "        loss = output.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if n_upd % 100 == 0:\n",
    "            # Calculate and log gradient norms\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "            print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Loss: {loss.item():.4f} (Recon: {output.loss_recon.item():.4f}, KL: {output.loss_kl.item():.4f}) Grad: {total_norm:.4f}')\n",
    "\n",
    "            if writer is not None:\n",
    "                global_step = n_upd\n",
    "                writer.add_scalar('Loss/Train', loss.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/BCE', output.loss_recon.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/KLD', output.loss_kl.item(), global_step)\n",
    "                writer.add_scalar('GradNorm/Train', total_norm, global_step)\n",
    "            \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, cur_step, writer=None):\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        cur_step (int): The current step.\n",
    "        writer: The TensorBoard writer.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader, desc='Testing'):\n",
    "            target = np.zeros(data.shape[0])            \n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)  # Flatten the data\n",
    "            \n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "            \n",
    "            test_loss += output.loss.item()\n",
    "            test_recon_loss += output.loss_recon.item()\n",
    "            test_kl_loss += output.loss_kl.item()\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_kl_loss /= len(dataloader)\n",
    "    print(f'====> Test set loss: {test_loss:.4f} (BCE: {test_recon_loss:.4f}, KLD: {test_kl_loss:.4f})')\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('Loss/Test', test_loss, global_step=cur_step)\n",
    "        writer.add_scalar('Loss/Test/BCE', output.loss_recon.item(), global_step=cur_step)\n",
    "        writer.add_scalar('Loss/Test/KLD', output.loss_kl.item(), global_step=cur_step)\n",
    "        \n",
    "        # Log reconstructions\n",
    "        writer.add_images('Test/Reconstructions', output.x_recon.view(-1, 1, 64, 64), global_step=cur_step)\n",
    "        writer.add_images('Test/Originals', data.view(-1, 1, 64, 64), global_step=cur_step)\n",
    "        \n",
    "        # Log random samples from the latent space\n",
    "        z = torch.randn(16, latent_dim).to(device)\n",
    "        samples = model.decode(z)\n",
    "        writer.add_images('Test/Samples', samples.view(-1, 1, 64, 64), global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    data.to_numpy(dtype=np.float32), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    data.to_numpy(dtype=np.float32), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5653 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of target should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     prev_updates \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test(model, test_loader, prev_updates, writer\u001b[38;5;241m=\u001b[39mwriter)\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, prev_updates, writer)\u001b[0m\n\u001b[1;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x, compute_loss)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VAEOutput(\n\u001b[1;32m     99\u001b[0m         z_dist\u001b[38;5;241m=\u001b[39mdist,\n\u001b[1;32m    100\u001b[0m         z_sample\u001b[38;5;241m=\u001b[39mz,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m         loss_kl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# compute loss terms \u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m loss_recon \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    109\u001b[0m std_normal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal(\n\u001b[1;32m    110\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros_like(z, device\u001b[38;5;241m=\u001b[39mz\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    111\u001b[0m     scale_tril\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39meye(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mz\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    113\u001b[0m loss_kl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mkl\u001b[38;5;241m.\u001b[39mkl_divergence(dist, std_normal)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of target should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "prev_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    prev_updates = train(model, train_loader, optimizer, prev_updates, writer=writer)\n",
    "    test(model, test_loader, prev_updates, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"chat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
